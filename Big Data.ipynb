{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you’ll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I'll continue to develop.* Thanks, and may you have \"statistically significant\" luck!\n\n\n# Table of Contents\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\n1. [Chapter 2 - A Data Science Framework](#ch2)\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\n1. [Chapter 5 - The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\n1. [Chapter 7 - Step 5: Model Data](#ch7)\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\n\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don't understand something in the code the print() function is your best friend. In coding, it's okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you've exhausted all your resources, the Kaggle Community via forums and comments can help too.","metadata":{"_cell_guid":"ea24845a-4a15-4235-9ed2-4823acbf4317","_uuid":"89022d89e1603ef3869a96b9da78da38b0b33b14"}},{"cell_type":"markdown","source":"<a id=\"ch1\"></a>\n# How a Data Scientist Beat the Odds\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle's Getting Started Competition, **Predict Future Sales**, to walk the reader through, how-to use the data science framework to beat the odds.\n   \n*What happens when technology is too smart for its own good?*","metadata":{"_cell_guid":"ec31d549-537e-4f35-8594-22ef03079081","_uuid":"ff6c70c884a8a5fb3258e7f3fefc9e55261df1df"}},{"cell_type":"markdown","source":"<a id=\"ch2\"></a>\n# A Data Science Framework\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are “drowning in data, yet staving for knowledge.\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \"dirty data\" to \"clean data.\"\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn “wild” data into “manageable” data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that’s used as actionable intelligence) at worst.\n6. **Validate and Implement Data Model:** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).\n7. **Optimize and Strategize:** This is the \"bionic man\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your “currency exchange\" rate.","metadata":{"_cell_guid":"8514a868-6816-494a-9e9c-dba14cb36f42","_uuid":"bd2a68dcaace0c1371f2994f0204dce65800cc20"}},{"cell_type":"markdown","source":"<a id=\"ch3\"></a>\n# Step 1: Define the Problem\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\n\n......\n\n**Project Summary:**\nProject Summary: In this competition you will work with a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company. \n\nWe are asking you to predict total sales for every product and store in the next month. By solving this competition you will be able to apply and enhance your data science skills.\n\n# Step 2: Gather the Data\nYou are provided with daily historical sales data. The task is to forecast the total amount of products sold in every shop for the test set. Note that the list of shops and products slightly changes every month. Creating a robust model that can handle such situations is part of the challenge.\n\nFile descriptions\n- sales_train.csv - the training set. Daily historical data from January 2013 to October 2015.\n- test.csv - the test set. You need to forecast the sales for these shops and products for November 2015.\n- sample_submission.csv - a sample submission file in the correct format.\n- items.csv - supplemental information about the items/products.\n- item_categories.csv  - supplemental information about the items categories.\n- shops.csv- supplemental information about the shops.\n\nThe dataset is also given to us on a golden plater with test and train data at [Predict Future Sales](https://www.kaggle.com/c/competitive-data-science-predict-future-sales/data)\n","metadata":{"_cell_guid":"22ba49b5-b3b6-4129-8c57-623e0d863ab3","_uuid":"ff014109d795c5d1bfcc66c9761730406124ebe4"}},{"cell_type":"markdown","source":"<a id=\"ch4\"></a>\n# Step 3: Prepare Data for Consumption\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\n\n## 3.1 Import Libraries\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. ","metadata":{"_cell_guid":"ef4059c6-9397-4b72-85da-570f51ba139c","_uuid":"f1d8b59b37d2ce04a5396a0ab183dc3000817113"}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n#load packages\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport matplotlib #collection of functions for scientific and publication-ready visualization\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\n\nimport numpy as np #foundational package for scientific computing\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport scipy as sp #collection of functions for scientific computing and advance mathematics\nprint(\"SciPy version: {}\". format(sp.__version__)) \n\nimport IPython\nfrom IPython import display #pretty printing of dataframes in Jupyter notebook\nprint(\"IPython version: {}\". format(IPython.__version__)) \n\nimport sklearn #collection of machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\n#misc libraries\nimport random\nimport time\nimport datetime # manipulating date formats\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('-'*25)\n\n\n\n# Input data files are available in the \"../input/competitive-data-science-predict-future-sales\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input/competitive-data-science-predict-future-sales\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_cell_guid":"a33744a8-08c7-4158-a22f-e8f2008a25b0","_kg_hide-input":false,"_kg_hide-output":false,"_uuid":"412b006d19b0209bd10ee4c6d15fdf59f8d2af38","execution":{"iopub.status.busy":"2022-02-04T08:42:27.789659Z","iopub.execute_input":"2022-02-04T08:42:27.78999Z","iopub.status.idle":"2022-02-04T08:42:28.714343Z","shell.execute_reply.started":"2022-02-04T08:42:27.789908Z","shell.execute_reply":"2022-02-04T08:42:28.713603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.11 Load Data Modelling Libraries\n\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.","metadata":{"_cell_guid":"a514e15b-8caa-4b2e-bb46-13521761ef27","_uuid":"2e4810f3c85ffdee7d7382167e455baf1c320bc8"}},{"cell_type":"code","source":"#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","metadata":{"_cell_guid":"b230c790-a3f5-46ed-b351-57e96cc8fa61","_uuid":"4a16d09256317518769f21bf9cc38726df8b0078","execution":{"iopub.status.busy":"2022-02-04T08:42:28.716708Z","iopub.execute_input":"2022-02-04T08:42:28.71738Z","iopub.status.idle":"2022-02-04T08:42:29.193123Z","shell.execute_reply.started":"2022-02-04T08:42:28.717334Z","shell.execute_reply":"2022-02-04T08:42:29.192337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Meet and Greet Data\n\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what's its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\n\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview of variable datatypes (i.e. qualitative vs quantitative). Click here for the [Source Data Dictionary](https://www.kaggle.com/c/competitive-data-science-predict-future-sales/data).\n\n **It's important to note, more predictor variables do not make a better model, but the right variables.**\n\nData fields\n1. ID - an Id that represents a (Shop, Item) tuple within the test set\n2. shop_id - unique identifier of a shop\n3. item_id - unique identifier of a product\n4. item_category_id - unique identifier of item category\n5. item_cnt_day - number of products sold. You are predicting a monthly amount of this measure\n6. item_price - current price of an item\n7. date - date in format dd/mm/yyyy\n8. date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n9. item_name - name of item\n10. shop_name - name of shop\n11. item_category_name - name of item category","metadata":{"_cell_guid":"c6289058-157a-46ba-b5c3-f134b268f5cc","_uuid":"a0fe0965de515e8c82cabd1fbd82cce993279f16"}},{"cell_type":"code","source":"#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\ndata_raw = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\n\n\n#a dataset should be broken into 3 splits: train, test, and (final) validation\n#the test file provided is the validation file for competition submission\n#we will split the train set into train and test data in future sections\ndata_val  = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\n\n\n#to play with our data we'll create a copy\n#remember python assignment or equal passes by reference vs values, so we use the copy function: https://stackoverflow.com/questions/46327494/python-pandas-dataframe-copydeep-false-vs-copydeep-true-vs\ndata1 = data_raw.copy(deep = True)\n\n#however passing by reference is convenient, because we can clean both datasets at once\ndata_cleaner = [data1, data_val]\n\n\n#preview data\n\nprint(\"\\n ----------Top-5- Record----------\")\nprint(data_raw.head(5))  #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html\n# print(data_raw.tail(5)) #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.html\n# print(data_raw.sample(10)) #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html\nprint(\"\\n -----------Information-----------\")\nprint(data_raw.info())  #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html\nprint(\"\\n -----------Data Types-----------\")\nprint(data_raw.dtypes)\nprint(\"\\n ----------Missing value-----------\")\nprint(data_raw.isnull().sum())\nprint(\"\\n ----------Null value-----------\")\nprint(data_raw.isna().sum())\nprint(\"\\n ----------Shape of Data----------\")\nprint(data_raw.shape)","metadata":{"_cell_guid":"c0928d7d-043b-4aa5-8113-e96fb42dbdc7","_uuid":"d0afb79a714ea826208235d05afac4e3f60554db","execution":{"iopub.status.busy":"2022-02-04T08:42:29.19631Z","iopub.execute_input":"2022-02-04T08:42:29.196588Z","iopub.status.idle":"2022-02-04T08:42:31.791212Z","shell.execute_reply.started":"2022-02-04T08:42:29.196554Z","shell.execute_reply":"2022-02-04T08:42:31.790283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n ----------Number of duplicates----------\")\nprint('Number of duplicates:', len(data_raw[data_raw.duplicated()]))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:42:31.792437Z","iopub.execute_input":"2022-02-04T08:42:31.792668Z","iopub.status.idle":"2022-02-04T08:42:32.568236Z","shell.execute_reply.started":"2022-02-04T08:42:31.792635Z","shell.execute_reply":"2022-02-04T08:42:32.567299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We have duplicated rows, but I don't think that it is a mistake.\nIt could be different sales methods or client type, etc.\n- You can remove it, but I really don't believe that 6 rows of 3m can make the difference.","metadata":{}},{"cell_type":"markdown","source":"### Lets group data by item_id and date_block_num and look closer on it.¶","metadata":{}},{"cell_type":"code","source":"sales_by_item_id = data_raw.pivot_table(index=['item_id'],values=['item_cnt_day'], \n                                        columns='date_block_num', aggfunc=np.sum, fill_value=0).reset_index()\nsales_by_item_id.columns = sales_by_item_id.columns.droplevel().map(str)\nsales_by_item_id = sales_by_item_id.reset_index(drop=True).rename_axis(None, axis=1)\nsales_by_item_id.columns.values[0] = 'item_id'","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:42:32.570182Z","iopub.execute_input":"2022-02-04T08:42:32.570417Z","iopub.status.idle":"2022-02-04T08:42:33.035367Z","shell.execute_reply.started":"2022-02-04T08:42:32.570388Z","shell.execute_reply":"2022-02-04T08:42:33.034584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see how many products are outdated (no sales for the last 6 months)\n\noutdated_items = sales_by_item_id[sales_by_item_id.loc[:,'27':].sum(axis=1)==0]\nprint('Outdated items:', len(outdated_items))\nprint('Outdated items in test set:', len(data_val[data_val['item_id'].isin(outdated_items['item_id'])]))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:42:33.036507Z","iopub.execute_input":"2022-02-04T08:42:33.03671Z","iopub.status.idle":"2022-02-04T08:42:33.052064Z","shell.execute_reply.started":"2022-02-04T08:42:33.036685Z","shell.execute_reply":"2022-02-04T08:42:33.051214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 12391 of 21807 is a huge number. Probably we can set 0 for all that items and do not make any model prediction.\n- in test set 6888 - not much but we have such items","metadata":{}},{"cell_type":"code","source":"# Outliers by price and sales volume\nprint('Sale volume outliers:',data_raw['item_id'][data_raw['item_cnt_day']>500].unique())\nprint('Item price outliers:',data_raw['item_id'][data_raw['item_price']>50000].unique())","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:42:33.053975Z","iopub.execute_input":"2022-02-04T08:42:33.055694Z","iopub.status.idle":"2022-02-04T08:42:33.072391Z","shell.execute_reply.started":"2022-02-04T08:42:33.05565Z","shell.execute_reply":"2022-02-04T08:42:33.071377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Possible item_id features:\n- Lags\n- Release date\n- Last month sale\n- Days on sale\n- Neighbors (items with id 1000 and 1001 could be somehow similar - genre, type, release date)","metadata":{}},{"cell_type":"markdown","source":"### Lets now group train data by shop_id.¶\nWe can see new shops - probably there will be a sales spike (opening event for example). Apparently closed shops (ill call it \"outdated shops\") - no sales for last 6 months.","metadata":{}},{"cell_type":"code","source":"sales_by_shop_id = data_raw.pivot_table(index=['shop_id'],values=['item_cnt_day'], \n                                        columns='date_block_num', aggfunc=np.sum, fill_value=0).reset_index()\nsales_by_shop_id.columns = sales_by_shop_id.columns.droplevel().map(str)\nsales_by_shop_id = sales_by_shop_id.reset_index(drop=True).rename_axis(None, axis=1)\nsales_by_shop_id.columns.values[0] = 'shop_id'\n\nprint(\"\\n ----------Dynamics of opening new stores (each line is a month, and the values are store numbers).----------\")\nfor i in range(6,34):\n    print('Not exists in month',i,sales_by_shop_id['shop_id'][sales_by_shop_id.loc[:,'0':str(i)].sum(axis=1)==0].unique())\n\nprint(\"\\n ----------Dynamics of outdated stores (each line is a month, and the values are store numbers).----------\")    \nfor i in range(6,28):\n    print('Shop is outdated for month',i,sales_by_shop_id['shop_id'][sales_by_shop_id.loc[:,str(i):].sum(axis=1)==0].unique())\n\nprint('Recently opened shop items:', len(data_raw[data_raw['shop_id']==36]))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:42:33.073955Z","iopub.execute_input":"2022-02-04T08:42:33.074235Z","iopub.status.idle":"2022-02-04T08:42:33.388054Z","shell.execute_reply.started":"2022-02-04T08:42:33.074196Z","shell.execute_reply":"2022-02-04T08:42:33.387249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In our test set we have 5100 sales in really new shop and no \"outdated shops\" but anyway it is good feature for future.\n\nPossible shop_id features:\n- Lags (shop_id/shp_cnt_mth)\n- Opening month (possible opening sales)\n- Closed Month (possible stock elimination)","metadata":{}},{"cell_type":"code","source":"good_sales = data_val.merge(data_raw, on=['item_id','shop_id'], how='left').dropna()\ngood_pairs = data_val[data_val['ID'].isin(good_sales['ID'])]\nno_data_items = data_val[~(data_val['item_id'].isin(data_raw['item_id']))]\n\nprint('1. Number of good pairs:', len(good_pairs))\nprint('2. No Data Items:', len(no_data_items))\nprint('3. Only Item_id Info:', len(data_val)-len(no_data_items)-len(good_pairs))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:42:33.389118Z","iopub.execute_input":"2022-02-04T08:42:33.389613Z","iopub.status.idle":"2022-02-04T08:42:34.081634Z","shell.execute_reply.started":"2022-02-04T08:42:33.389571Z","shell.execute_reply":"2022-02-04T08:42:34.081083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ch5\"></a>\n## 3.21 The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs.  We will wait until after we complete our exploratory analysis to determine if we should include or exclude from the dataset. It should be noted, that if they were unreasonable values, then it's probably a safe decision to fix now. However, we want to use caution when we modify data from its original value, because it may be necessary to create an accurate model.\n\n2. **Completing:** Missing values can be bad, because some algorithms don't know how-to handle null values and will fail. While others, like decision trees, can handle null values. Thus, it's important to fix before we start modeling, because we will compare and contrast several models. There are two common methods, either delete the record or populate the missing value using a reasonable input. It is not recommended to delete the record, especially a large percentage of records, unless it truly represents an incomplete record. Instead, it's best to impute missing values. A basic methodology for qualitative data is impute using mode. A basic methodology for quantitative data is impute using mean, median, or mean + randomized standard deviation. An intermediate methodology is to use the basic methodology based on specific criteria. There are more complex methodologies, however before deploying, it should be compared to the base model to determine if complexity truly adds value. \n\n3. **Creating:**  Feature engineering is when we use existing features to create new features to determine if they provide new signals to predict our outcome. \n\n4. **Converting:** Last but not least, we'll take care of the formatting. Always date or currency formats, or data type formats must be converted to a form convenient for our model to work with. Often categorical data is imported as objects, which makes mathematical calculations difficult and then it is necessary to convert the object data types into categorical dummy variables.","metadata":{"_cell_guid":"b575d2d8-b77d-4810-9232-3b2726145ca8","_uuid":"281ad467b485855c1a39116c5ed899ac3018689f"}},{"cell_type":"code","source":"print('Train columns with null values:\\n', data1.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')","metadata":{"_cell_guid":"e2181a68-3c33-4040-83a9-99adee8363a0","_kg_hide-input":false,"_kg_hide-output":false,"_uuid":"9a36681f0eda8ccc6396c64aa6bd6e8288461bcb","execution":{"iopub.status.busy":"2022-02-04T08:42:34.082731Z","iopub.execute_input":"2022-02-04T08:42:34.083096Z","iopub.status.idle":"2022-02-04T08:42:34.842074Z","shell.execute_reply.started":"2022-02-04T08:42:34.083067Z","shell.execute_reply":"2022-02-04T08:42:34.841044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.22 Clean Data\n\nNow that we know what to clean, let's execute our code.\n\n** Developer Documentation: **\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-docs/stable/indexing.html)\n* [pandas.isnull](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html)\n* [pandas.DataFrame.sum](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sum.html)\n* [pandas.DataFrame.mode](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mode.html)\n* [pandas.DataFrame.copy](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.copy.html)\n* [pandas.DataFrame.fillna](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html)\n* [pandas.DataFrame.drop](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html)\n* [pandas.Series.value_counts](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html)\n* [pandas.DataFrame.loc](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html)","metadata":{"_cell_guid":"c6fc9f66-8779-454c-b6bd-47676c0dc81c","_uuid":"5002c97888cfdcb2b49ef76fd1505eed6a1721e4"}},{"cell_type":"code","source":"'''pay attention to the format of the source data in the column date before processing'''\nprint(data_raw.info())","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:42:34.843468Z","iopub.execute_input":"2022-02-04T08:42:34.843779Z","iopub.status.idle":"2022-02-04T08:42:34.856598Z","shell.execute_reply.started":"2022-02-04T08:42:34.843741Z","shell.execute_reply":"2022-02-04T08:42:34.855591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#formatting the date column correctly\ndata_raw.date=data_raw.date.apply(lambda x:datetime.datetime.strptime(x, '%d.%m.%Y'))\n# check\n'''pay attention to the data format in the date column after processing has changed and became datetime64[ns]'''\nprint(data_raw.info())","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:42:34.857764Z","iopub.execute_input":"2022-02-04T08:42:34.858168Z","iopub.status.idle":"2022-02-04T08:42:58.33612Z","shell.execute_reply.started":"2022-02-04T08:42:34.858139Z","shell.execute_reply":"2022-02-04T08:42:58.3352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Time period of the dataset\nprint('Min date from train set: %s' % data_raw['date'].min().date())\nprint('Max date from train set: %s' % data_raw['date'].max().date())","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:42:58.33769Z","iopub.execute_input":"2022-02-04T08:42:58.33797Z","iopub.status.idle":"2022-02-04T08:42:58.361916Z","shell.execute_reply.started":"2022-02-04T08:42:58.33793Z","shell.execute_reply":"2022-02-04T08:42:58.361348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data leakages\n# About data leakages I'll only be using only the \"shop_id\" and \"item_id\" that appear on the test set\ntest_shop_ids = data_raw['shop_id'].unique()\ntest_item_ids = data_val['item_id'].unique()\n# Only shops that exist in test set.\nlk_train = data_raw[data_raw['shop_id'].isin(test_shop_ids)]\n# Only items that exist in test set.\nlk_train = lk_train[lk_train['item_id'].isin(test_item_ids)]\n\nprint('Data set size before leaking:', data_raw.shape[0])\nprint('Data set size after leaking:', lk_train.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:42:58.364075Z","iopub.execute_input":"2022-02-04T08:42:58.364766Z","iopub.status.idle":"2022-02-04T08:42:58.627808Z","shell.execute_reply.started":"2022-02-04T08:42:58.364732Z","shell.execute_reply":"2022-02-04T08:42:58.626972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data cleaning¶\nprint('all price      ', len(data_raw))\nprint('positive price ', len(data_raw.query('item_price>0')))\nprint('negative price ', len(data_raw.query('item_price<0')))\ndata_raw = data_raw.query('item_price > 0')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:42:58.629267Z","iopub.execute_input":"2022-02-04T08:42:58.631824Z","iopub.status.idle":"2022-02-04T08:42:58.865696Z","shell.execute_reply.started":"2022-02-04T08:42:58.631791Z","shell.execute_reply":"2022-02-04T08:42:58.864926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data preprocessing\n- dropping the text features\n- We are asked to predict total sales for every product and store in the next month, and our data is given by day, so let's remove unwanted columns and aggregate the data by month.","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv', dtype={'ID': 'int32', 'shop_id': 'int32', \n                                                  'item_id': 'int32'})\nitem_categories = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv', \n                              dtype={'item_category_name': 'str', 'item_category_id': 'int32'})\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv', dtype={'item_name': 'str', 'item_id': 'int32', \n                                                 'item_category_id': 'int32'})\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv', dtype={'shop_name': 'str', 'shop_id': 'int32'})\nsales = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv', parse_dates=['date'], \n                    dtype={'date': 'str', 'date_block_num': 'int32', 'shop_id': 'int32', \n                          'item_id': 'int32', 'item_price': 'float32', 'item_cnt_day': 'int32'})\n\n# Join data sets\ntrain = sales.join(items, on='item_id', rsuffix='_').join(shops, on='shop_id', rsuffix='_').join(item_categories, on='item_category_id', rsuffix='_').drop(['item_id_', 'shop_id_', 'item_category_id_'], axis=1)\n\ndef downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df\n\ntrain = downcast_dtypes(train)\ntest = downcast_dtypes(test)\n\n# Data leakages\n# About data leakages I'll only be using only the \"shop_id\" and \"item_id\" that appear on the test set.\ntest_shop_ids = test['shop_id'].unique()\ntest_item_ids = test['item_id'].unique()\n# Only shops that exist in test set.\nlk_train = train[train['shop_id'].isin(test_shop_ids)]\n# Only items that exist in test set.\nlk_train = lk_train[lk_train['item_id'].isin(test_item_ids)]\ntrain = train.query('item_price > 0')\n\n\n# Select only useful features.\ntrain_monthly = lk_train[['date', 'date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'item_cnt_day']]\n\n# Group by month in this case \"date_block_num\" and aggregate features.\ntrain_monthly = train_monthly.sort_values('date').groupby(['date_block_num', 'shop_id', 'item_category_id', 'item_id'], as_index=False)\ntrain_monthly = train_monthly.agg({'item_price':['sum', 'mean'], 'item_cnt_day':['sum', 'mean','count']})\n# Rename features.\ntrain_monthly.columns = ['date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'mean_item_price', 'item_cnt', 'mean_item_cnt', 'transactions']","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:42:58.866854Z","iopub.execute_input":"2022-02-04T08:42:58.867262Z","iopub.status.idle":"2022-02-04T08:43:03.167826Z","shell.execute_reply.started":"2022-02-04T08:42:58.867231Z","shell.execute_reply":"2022-02-04T08:43:03.166995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To mimic the real behavior of the data we have to create the missing records from the loaded dataset, so for each month we need to create the missing records for each shop and item\nprint(\"\\n ----------Missing value-----------\")\nprint(train_monthly.isnull().sum())\nprint(\"\\n ----------Null value-----------\")\nprint(train_monthly.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:43:03.169047Z","iopub.execute_input":"2022-02-04T08:43:03.169237Z","iopub.status.idle":"2022-02-04T08:43:03.194873Z","shell.execute_reply.started":"2022-02-04T08:43:03.169213Z","shell.execute_reply":"2022-02-04T08:43:03.193714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build a data set with all the possible combinations of ['date_block_num','shop_id','item_id'] so we won't have missing records.\nshop_ids = train_monthly['shop_id'].unique()\nitem_ids = train_monthly['item_id'].unique()\nempty_df = []\nfor i in range(34):\n    for shop in shop_ids:\n        for item in item_ids:\n            empty_df.append([i, shop, item])\n    \nempty_df = pd.DataFrame(empty_df, columns=['date_block_num','shop_id','item_id'])","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:43:03.195929Z","iopub.execute_input":"2022-02-04T08:43:03.196148Z","iopub.status.idle":"2022-02-04T08:43:15.214366Z","shell.execute_reply.started":"2022-02-04T08:43:03.196121Z","shell.execute_reply":"2022-02-04T08:43:15.213697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge the train set with the complete set (missing records will be filled with 0).\ntrain_monthly = pd.merge(empty_df, train_monthly, on=['date_block_num','shop_id','item_id'], how='left')\ntrain_monthly.fillna(0, inplace=True)\n\n# Extract time based features.\ntrain_monthly['year'] = train_monthly['date_block_num'].apply(lambda x: ((x//12) + 2013))\ntrain_monthly['month'] = train_monthly['date_block_num'].apply(lambda x: (x % 12))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:43:15.21535Z","iopub.execute_input":"2022-02-04T08:43:15.215562Z","iopub.status.idle":"2022-02-04T08:43:22.108882Z","shell.execute_reply.started":"2022-02-04T08:43:15.215536Z","shell.execute_reply":"2022-02-04T08:43:22.108043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### reduce the amount of data\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df\n\nreduce_mem_usage(train_monthly)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:43:22.109992Z","iopub.execute_input":"2022-02-04T08:43:22.11021Z","iopub.status.idle":"2022-02-04T08:43:22.786188Z","shell.execute_reply.started":"2022-02-04T08:43:22.110183Z","shell.execute_reply":"2022-02-04T08:43:22.785401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing outliers\n# I'll treat \"item_cnt\" > 20 and < 0, \"item_price\" >= 400000 as outliers, so I'll remove them.\ntrain_monthly = train_monthly.query('item_cnt >= 0 and item_cnt <= 20 and item_price < 400000')\n\n# Creating the label\n# Our label will be the \"item_cnt\" of the next month, as we are dealing with a forecast problem.\ntrain_monthly['item_cnt_month'] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_id'])['item_cnt'].shift(-1)\n\n# Feature engineering\n# Unitary item prices.\ntrain_monthly['item_price_unit'] = train_monthly['item_price'] // train_monthly['item_cnt']\ntrain_monthly['item_price_unit'].fillna(0, inplace=True)\n\n# Group based features\ngp_item_price = train_monthly.sort_values('date_block_num').groupby(['item_id'], as_index=False).agg({'item_price':[np.min, np.max]})\ngp_item_price.columns = ['item_id', 'hist_min_item_price', 'hist_max_item_price']\n\ntrain_monthly = pd.merge(train_monthly, gp_item_price, on='item_id', how='left')\n\n# How much each item's price changed from its (lowest/highest) historical price.\ntrain_monthly['price_increase'] = train_monthly['item_price'] - train_monthly['hist_min_item_price']\ntrain_monthly['price_decrease'] = train_monthly['hist_max_item_price'] - train_monthly['item_price']\n\n\nprint('Rolling window based features')\n# Rolling window based features (window = 3 months)\n# Min value\nf_min = lambda x: x.rolling(window=3, min_periods=1).min()\n# Max value\nf_max = lambda x: x.rolling(window=3, min_periods=1).max()\n# Mean value\nf_mean = lambda x: x.rolling(window=3, min_periods=1).mean()\n# Standard deviation\nf_std = lambda x: x.rolling(window=3, min_periods=1).std()\n\nfunction_list = [f_min, f_max, f_mean, f_std]\nfunction_name = ['min', 'max', 'mean', 'std']\n\nfor i in range(len(function_list)):\n    train_monthly[('item_cnt_%s' % function_name[i])] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt'].apply(function_list[i])\n    print('function', i)\n    \n# Fill the empty std features with 0\ntrain_monthly['item_cnt_std'].fillna(0, inplace=True)\n\n# Lag based features\nlag_list = [1, 2, 3]\n\nfor lag in lag_list:\n    ft_name = ('item_cnt_shifted%s' % lag)\n    train_monthly[ft_name] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt'].shift(lag)\n    # Fill the empty shifted features with 0\n    train_monthly[ft_name].fillna(0, inplace=True)\n    \n# Item sales count trend\ntrain_monthly['item_trend'] = train_monthly['item_cnt']\n\nfor lag in lag_list:\n    ft_name = ('item_cnt_shifted%s' % lag)\n    train_monthly['item_trend'] -= train_monthly[ft_name]\n\ntrain_monthly['item_trend'] /= len(lag_list) + 1\n\ntrain_set = train_monthly.query('date_block_num >= 3 and date_block_num < 28').copy()\nvalidation_set = train_monthly.query('date_block_num >= 28 and date_block_num < 33').copy()\ntest_set = train_monthly.query('date_block_num == 33').copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:43:22.787637Z","iopub.execute_input":"2022-02-04T08:43:22.788085Z","iopub.status.idle":"2022-02-04T08:49:36.585076Z","shell.execute_reply.started":"2022-02-04T08:43:22.788043Z","shell.execute_reply":"2022-02-04T08:49:36.584314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set.dropna(subset=['item_cnt_month'], inplace=True)\nvalidation_set.dropna(subset=['item_cnt_month'], inplace=True)\n\ntrain_set.dropna(inplace=True)\nvalidation_set.dropna(inplace=True)\n\nprint('Train set records:', train_set.shape[0])\nprint('Validation set records:', validation_set.shape[0])\nprint('Test set records:', test_set.shape[0])\n\nprint('Train set records: %s (%.f%% of complete data)' % (train_set.shape[0], ((train_set.shape[0]/train_monthly.shape[0])*100)))\nprint('Validation set records: %s (%.f%% of complete data)' % (validation_set.shape[0], ((validation_set.shape[0]/train_monthly.shape[0])*100)))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:49:36.586487Z","iopub.execute_input":"2022-02-04T08:49:36.586753Z","iopub.status.idle":"2022-02-04T08:49:38.678273Z","shell.execute_reply.started":"2022-02-04T08:49:36.586723Z","shell.execute_reply":"2022-02-04T08:49:38.677544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shop mean encoding.\ngp_shop_mean = train_set.groupby(['shop_id']).agg({'item_cnt_month': ['mean']})\ngp_shop_mean.columns = ['shop_mean']\ngp_shop_mean.reset_index(inplace=True)\n# Item mean encoding.\ngp_item_mean = train_set.groupby(['item_id']).agg({'item_cnt_month': ['mean']})\ngp_item_mean.columns = ['item_mean']\ngp_item_mean.reset_index(inplace=True)\n# Shop with item mean encoding.\ngp_shop_item_mean = train_set.groupby(['shop_id', 'item_id']).agg({'item_cnt_month': ['mean']})\ngp_shop_item_mean.columns = ['shop_item_mean']\ngp_shop_item_mean.reset_index(inplace=True)\n# Year mean encoding.\ngp_year_mean = train_set.groupby(['year']).agg({'item_cnt_month': ['mean']})\ngp_year_mean.columns = ['year_mean']\ngp_year_mean.reset_index(inplace=True)\n# Month mean encoding.\ngp_month_mean = train_set.groupby(['month']).agg({'item_cnt_month': ['mean']})\ngp_month_mean.columns = ['month_mean']\ngp_month_mean.reset_index(inplace=True)\n\n# Add meand encoding features to train set.\ntrain_set = pd.merge(train_set, gp_shop_mean, on=['shop_id'], how='left')\ntrain_set = pd.merge(train_set, gp_item_mean, on=['item_id'], how='left')\ntrain_set = pd.merge(train_set, gp_shop_item_mean, on=['shop_id', 'item_id'], how='left')\ntrain_set = pd.merge(train_set, gp_year_mean, on=['year'], how='left')\ntrain_set = pd.merge(train_set, gp_month_mean, on=['month'], how='left')\n\n# Add meand encoding features to validation set.\nvalidation_set = pd.merge(validation_set, gp_shop_mean, on=['shop_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_item_mean, on=['item_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_shop_item_mean, on=['shop_id', 'item_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_year_mean, on=['year'], how='left')\nvalidation_set = pd.merge(validation_set, gp_month_mean, on=['month'], how='left')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:49:38.679252Z","iopub.execute_input":"2022-02-04T08:49:38.679432Z","iopub.status.idle":"2022-02-04T08:49:48.533805Z","shell.execute_reply.started":"2022-02-04T08:49:38.679408Z","shell.execute_reply":"2022-02-04T08:49:48.532894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.23 Convert Formats\n\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\n\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\n\n** Developer Documentation: **\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\n* [Sklearn LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n* [Sklearn OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n* [Pandas Categorical dtype](https://pandas.pydata.org/pandas-docs/stable/categorical.html)\n* [pandas.get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)","metadata":{"_cell_guid":"f7972928-84b5-47b1-98b6-3e47bc7ddb17","_uuid":"eb0a15c2065a827c3c5431622c04707f3e9d2e52"}},{"cell_type":"code","source":"#define y variable aka target/outcome\nTarget = ['item_cnt_month']\n\n# data1_x = ['date_block_num', 'shop_id', 'item_id', 'item_category_id',\n#        'item_price', 'mean_item_price', 'item_cnt', 'mean_item_cnt',\n#        'transactions', 'year', 'month', 'item_price_unit',\n#        'hist_min_item_price', 'hist_max_item_price', 'price_increase',\n#        'price_decrease', 'item_cnt_min', 'item_cnt_max', 'item_cnt_mean',\n#        'item_cnt_std', 'item_cnt_shifted1', 'item_cnt_shifted2',\n#        'item_cnt_shifted3', 'item_trend', 'shop_mean', 'item_mean',\n#        'shop_item_mean', 'year_mean', 'month_mean']\n\n#define x variables for original features aka feature selection\ndata1_x = ['date_block_num', 'shop_id', 'item_id', 'item_category_id', 'item_price', 'item_cnt', 'year', 'month'] #pretty name/values for charts\n\ndata1_x_calc = ['date_block_num', 'shop_id', 'item_id', 'item_category_id', 'item_price', 'item_cnt', 'year', 'month'] #coded for algorithm calculation\ndata1_xy =  Target + data1_x\nprint('Original X Y: ', data1_xy, '\\n')\n\n\n#define x variables for original w/bin features to remove continuous variables\ndata1_x_bin = ['date_block_num', 'shop_id', 'item_id', 'item_category_id', 'item_price', 'item_cnt', 'year', 'month']\ndata1_xy_bin = Target + data1_x_bin\nprint('Bin X Y: ', data1_xy_bin, '\\n')\n\n\n#define x and y variables for dummy features original\ndata1_dummy = pd.get_dummies(train_set[data1_x])\ndata1_x_dummy = data1_dummy.columns.tolist()\ndata1_xy_dummy = Target + data1_x_dummy\nprint('Dummy X Y: ', data1_xy_dummy, '\\n')\n\n\ndata1_dummy.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:49:48.534886Z","iopub.execute_input":"2022-02-04T08:49:48.535129Z","iopub.status.idle":"2022-02-04T08:49:48.970279Z","shell.execute_reply.started":"2022-02-04T08:49:48.535102Z","shell.execute_reply":"2022-02-04T08:49:48.969412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.24 Da-Double Check Cleaned Data\n\nNow that we've cleaned our data, let's do a discount da-double check!","metadata":{"_cell_guid":"43ae9ddf-85b6-4abd-89a7-cfb0ff70ebad","_uuid":"9f4b880b2e485e6bc4b6a64113b8f816e2319e85"}},{"cell_type":"code","source":"# print('Train columns with null values: \\n', data1.isnull().sum())\n# print(\"-\"*10)\n# print (data1.info())\n# print(\"-\"*10)\n\n# print('Test/Validation columns with null values: \\n', data_val.isnull().sum())\n# print(\"-\"*10)\n# print (data_val.info())\n# print(\"-\"*10)\n\n# data_raw.describe(include = 'all')","metadata":{"_cell_guid":"00d10c37-5d8a-4eaf-8fe8-25a0b58e0bfd","_uuid":"e2ef29db7e31dc83c10238ee33ad4ad0ad426183","execution":{"iopub.status.busy":"2022-02-04T08:49:48.971678Z","iopub.execute_input":"2022-02-04T08:49:48.97205Z","iopub.status.idle":"2022-02-04T08:49:48.976836Z","shell.execute_reply.started":"2022-02-04T08:49:48.972008Z","shell.execute_reply":"2022-02-04T08:49:48.975678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.25 Split Training and Testing Data\n\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don't [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the same dataset. It's important our algorithm has not seen the subset we will use to test, so it doesn't \"cheat\" by memorizing the answers. We will use [*sklearn's* train_test_split function](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). In later sections we will also use [*sklearn's* cross validation functions](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation), that splits our dataset into train and test for data modeling comparison.","metadata":{"_cell_guid":"e1ffcb68-17a5-43e0-b7e1-4a0b9d33dbdd","_uuid":"9b4b6e1f40e310274447c880760b87ec6a0c7c77"}},{"cell_type":"code","source":"# train_set.columns\n# validation_set.columns\ndata1= train_set.copy(deep = True)\ndata_val=validation_set.copy(deep = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:49:48.97797Z","iopub.execute_input":"2022-02-04T08:49:48.978255Z","iopub.status.idle":"2022-02-04T08:49:49.218337Z","shell.execute_reply.started":"2022-02-04T08:49:48.978218Z","shell.execute_reply":"2022-02-04T08:49:49.217571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#split train and test data with function defaults\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)\n\n\nprint(\"Data1 Shape: {}\".format(data1.shape))\nprint(\"Train1 Shape: {}\".format(train1_x.shape))\nprint(\"Test1 Shape: {}\".format(test1_x.shape))\n\ntrain1_x_bin.head()","metadata":{"_cell_guid":"15e84f99-e015-4e0a-bd0a-2f856ffad1f6","_uuid":"f9eb9f6e78235a38580ee3125ede17b836edabb3","execution":{"iopub.status.busy":"2022-02-04T08:49:49.219603Z","iopub.execute_input":"2022-02-04T08:49:49.219892Z","iopub.status.idle":"2022-02-04T08:49:52.824758Z","shell.execute_reply.started":"2022-02-04T08:49:49.219853Z","shell.execute_reply":"2022-02-04T08:49:52.823944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ch6\"></a>\n# Step 4: Perform Exploratory Analysis with Statistics\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.","metadata":{"_cell_guid":"8a13c80c-9f6b-405a-bba2-d3fb9b1cf19d","_uuid":"d3d49dc8106082d149dc4058b002355fc809726b"}},{"cell_type":"code","source":"#Discrete Variable Correlation by Survival using\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\nfor x in data1_x:\n    if data1[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')\n        \n\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\n# print(pd.crosstab(data1['Title'],data1[Target[0]]))\n","metadata":{"_cell_guid":"f14819d3-e23c-40fa-a281-375e8777a47b","_uuid":"5355b0e45b130a1fd574510141f65e62ebfe2148","execution":{"iopub.status.busy":"2022-02-04T08:49:52.826204Z","iopub.execute_input":"2022-02-04T08:49:52.826895Z","iopub.status.idle":"2022-02-04T08:49:54.386859Z","shell.execute_reply.started":"2022-02-04T08:49:52.826848Z","shell.execute_reply":"2022-02-04T08:49:54.386002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data1)","metadata":{"_cell_guid":"fbfaf416-4eb7-45fc-8ee2-123852ee6a7d","_uuid":"d599e6d35f40e66f3f2bd162b2d9087d10ed443d","execution":{"iopub.status.busy":"2022-02-04T08:49:54.388116Z","iopub.execute_input":"2022-02-04T08:49:54.388598Z","iopub.status.idle":"2022-02-04T08:50:11.809734Z","shell.execute_reply.started":"2022-02-04T08:49:54.388554Z","shell.execute_reply":"2022-02-04T08:50:11.808887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ch7\"></a>\n# Step 5: Model Data\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don’t worry, we only need a high-level overview, which we’ll cover in this Kernel. Also, thanks to computer science, a lot of the heavy lifting is done for you. So, problems that once required graduate degrees in mathematics or statistics, now only take a few lines of code. Last, we’ll need some business acumen to think through the problem. After all, like training a sight-seeing dog, it’s learning from us and not the other way around.\n\nMachine Learning (ML), as the name suggest, is teaching the machine how-to think and not what to think. While this topic and big data has been around for decades, it is becoming more popular than ever because the barrier to entry is lower, for businesses and professionals alike. This is both good and bad. It’s good because these algorithms are now accessible to more people that can solve more problems in the real-world. It’s bad because a lower barrier to entry means, more people will not know the tools they are using and can come to incorrect conclusions. That’s why I focus on teaching you, not just what to do, but why you’re doing it. Previously, I used the analogy of asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible; or even worst, implements incorrect actionable intelligence. So now that I’ve hammered (no pun intended) my point, I’ll show you what to do and most importantly, WHY you do it.\n\nFirst, you must understand, that the purpose of machine learning is to solve human problems. Machine learning can be categorized as: supervised learning, unsupervised learning, and reinforced learning. Supervised learning is where you train the model by presenting it a training dataset that includes the correct answer. Unsupervised learning is where you train the model using a training dataset that does not include the correct answer. And reinforced learning is a hybrid of the previous two, where the model is not given the correct answer immediately, but later after a sequence of events to reinforce learning. We are doing supervised machine learning, because we are training our algorithm by presenting it with a set of features and their corresponding target. We then hope to present it a new subset from the same dataset and have similar results in prediction accuracy.\n\nThere are many machine learning algorithms, however they can be reduced to four categories: classification, regression, clustering, or dimensionality reduction, depending on your target variable and data modeling goals. We'll save clustering and dimension reduction for another day, and focus on classification and regression. We can generalize that a continuous target variable requires a regression algorithm and a discrete target variable requires a classification algorithm. One side note, logistic regression, while it has regression in the name, is really a classification algorithm. Since our problem is predicting if a passenger survived or did not survive, this is a discrete target variable. We will use a classification algorithm from the *sklearn* library to begin our analysis. We will use cross validation and scoring metrics, discussed in later sections, to rank and compare our algorithms’ performance.\n\n**Machine Learning Selection:**\n* [Sklearn Estimator Overview](http://scikit-learn.org/stable/user_guide.html)\n* [Sklearn Estimator Detail](http://scikit-learn.org/stable/modules/classes.html)\n* [Choosing Estimator Mind Map](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n* [Choosing Estimator Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf)\n\n\nNow that we identified our solution as a supervised learning classification algorithm. We can narrow our list of choices.\n\n**Machine Learning Classification Algorithms:**\n* [Ensemble Methods](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)\n* [Generalized Linear Models (GLM)](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)\n* [Naive Bayes](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes)\n* [Nearest Neighbors](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors)\n* [Support Vector Machines (SVM)](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm)\n* [Decision Trees](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree)\n* [Discriminant Analysis](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.discriminant_analysis)\n\n\n### Data Science 101: How to Choose a Machine Learning Algorithm (MLA)\n**IMPORTANT:** When it comes to data modeling, the beginner’s question is always, \"what is the best machine learning algorithm?\" To this the beginner must learn, the [No Free Lunch Theorem (NFLT)](http://robertmarks.org/Classes/ENGR5358/Papers/NFL_4_Dummies.pdf) of Machine Learning. In short, NFLT states, there is no super algorithm, that works best in all situations, for all datasets. So the best approach is to try multiple MLAs, tune them, and compare them for your specific scenario. With that being said, some good research has been done to compare algorithms, such as [Caruana & Niculescu-Mizil 2006](https://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml06.pdf) watch [video lecture here](http://videolectures.net/solomon_caruana_wslmw/) of MLA comparisons, [Ogutu et al. 2011](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3103196/) done by the NIH for genomic selection, [Fernandez-Delgado et al. 2014](http://jmlr.org/papers/volume15/delgado14a/delgado14a.pdf) comparing 179 classifiers from 17 families, [Thoma 2016 sklearn comparison](https://martin-thoma.com/comparing-classifiers/), and there is also a school of thought that says, [more data beats a better algorithm](https://www.kdnuggets.com/2015/06/machine-learning-more-data-better-algorithms.html). \n\nSo with all this information, where is a beginner to start? I recommend starting with [Trees, Bagging, Random Forests, and Boosting](http://jessica2.msri.org/attachments/10778/10778-boost.pdf). They are basically different implementations of a decision tree, which is the easiest concept to learn and understand. They are also easier to tune, discussed in the next section, than something like SVC. Below, I'll give an overview of how-to run and compare several MLAs, but the rest of this Kernel will focus on learning data modeling via decision trees and its derivatives.","metadata":{"_cell_guid":"079d255c-e55e-482b-be9f-ae6b23015790","_uuid":"1f4f5f75093fdcec864a835d94cd5d9a05b591c5"}},{"cell_type":"code","source":"# data1.isnull().any()\n# data1 = data1.fillna(lambda x: x.median())\n# data1.describe()\n# data1.info()\n# data1.replace([np.inf, -np.inf], np.nan, inplace=True)\n# data1.isnull().sum()\n# data1[Target].describe()\n# data1[data1_x_bin].replace([np.inf, -np.inf], np.nan, inplace=True)\n# data1[data1_x_bin].describe()\n\n\nreduce_mem_usage(data1)\n# reduce_mem_usage(data1[data1_x_bin])\n","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:50:11.81105Z","iopub.execute_input":"2022-02-04T08:50:11.81125Z","iopub.status.idle":"2022-02-04T08:50:15.095282Z","shell.execute_reply.started":"2022-02-04T08:50:11.811226Z","shell.execute_reply":"2022-02-04T08:50:15.09429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(), # 1h +\n    ensemble.BaggingClassifier(), # 1h +\n#     ensemble.ExtraTreesClassifier(), # - allocate more memory \n#     ensemble.GradientBoostingClassifier(),  # 4h> ???\n#     ensemble.RandomForestClassifier(), # - allocate more memory \n\n    #Gaussian Processes\n#     gaussian_process.GaussianProcessClassifier(),  # - allocate more memory\n    \n    #GLM\n#     linear_model.LogisticRegressionCV(),  # 1h - allocate more memory\n    linear_model.PassiveAggressiveClassifier(),  # 0.7h +\n    linear_model.RidgeClassifierCV(),  # 0.5h +\n#     linear_model.SGDClassifier(),  # 4h> ???\n    linear_model.Perceptron(),  # +\n    \n#     #Navies Bayes\n    naive_bayes.BernoulliNB(), # +\n    naive_bayes.GaussianNB(),  # +\n    \n#     #Nearest Neighbor\n#     neighbors.KNeighborsClassifier(),  # more 1h -\n    \n#     #SVM\n#     svm.SVC(probability=True),  # more 2h -\n#     svm.NuSVC(probability=True),  # specified nu is infeasible -\n#     svm.LinearSVC(),  # more 2h -\n    \n#     #Trees    \n    tree.DecisionTreeClassifier(),  # +\n    tree.ExtraTreeClassifier(),  # \n    \n#     #Discriminant Analysis\n#     discriminant_analysis.LinearDiscriminantAnalysis(),\n#     discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n#     #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n#     XGBClassifier()    \n    ]\n\n\n\n#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n#note: this is an alternative to train_test_split\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\nprint('Split done!')\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = data1[Target]\n\nprint('Start prediction')\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:    \n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n#     cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv  = cv_split)\n    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv = cv_split, return_train_score=True)\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()    \n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!  \n    \n    \n    #save MLA predictions - see section 6 for usage\n    alg.fit(data1[data1_x_bin], data1[Target])\n    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n    \n    row_index+=1\n    print('Complite train ', row_index, alg)\n    \n#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare\n#MLA_predict","metadata":{"_cell_guid":"68816e33-19bc-482e-b4e8-062098b18798","_uuid":"979ae81e95cf760f05e94853b932b5101198b4e9","execution":{"iopub.status.busy":"2022-02-04T08:50:15.096712Z","iopub.execute_input":"2022-02-04T08:50:15.096953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","metadata":{"_cell_guid":"b9653051-8e79-4ae3-ac31-497bf0a5caed","_uuid":"b7acbf3f7ad6e812a2b5d60794bc3c5ad7d3fd0c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### References¶\nI hope you liked this code, I also prepared more interesting laptops for this competition and I will be glad to share them with you:\n\n1. [Data ScienceTutorial for Beginners](https://www.kaggle.com/andrej0marinchenko/data-sciencetutorial-for-beginners-predict-fs)\n2. [Step by Step for Beginners](https://www.kaggle.com/andrej0marinchenko/future-sales-step-by-step-for-beginners)\n3. [Future sales with automated ensembling](https://www.kaggle.com/andrej0marinchenko/future-sales-with-automated-ensembling)\n4. [Predict Future Sales LightGBM framework](https://www.kaggle.com/andrej0marinchenko/predict-future-sales-lightgbm-framework)\n5. [universal notebook for data analysis](https://www.kaggle.com/andrej0marinchenko/universal-notebook-for-data-analysis)","metadata":{}},{"cell_type":"markdown","source":"\n","metadata":{"_cell_guid":"51f197c2-76d7-4503-ade5-f9f35323c4c5","_uuid":"2e95a403934cf5c0ab1ff9148b62ef14590e84c9"}}]}